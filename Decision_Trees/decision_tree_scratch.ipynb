{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decison Tree from Scratch!\n",
    "We will be using Kaggle's [Spotify Song Attributes](https://www.kaggle.com/geomack/spotifyclassification/home) dataset. The dataset contains a number of features of songs from 2017 and a binary target variable representing whether the user liked the song or not. See the documentation of all the features from [here](https://developer.spotify.com/documentation/web-api/reference/tracks/get-audio-features/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    \"\"\"\n",
    "    A helper function that computes the entropy of the \n",
    "    discrete distribution p (stored in a 1D numpy array).\n",
    "    The elements of p should add up to 1.\n",
    "    This function ensures lim p-->0 of p log(p) = 0\n",
    "    which is mathematically true, but numerically results in NaN\n",
    "    because log(0) returns -Inf.\n",
    "    \"\"\"\n",
    "    plogp = 0*p # initialize full of zeros\n",
    "    plogp[p>0] = p[p>0]*np.log(p[p>0]) # only do the computation when p>0\n",
    "    return -np.sum(plogp)\n",
    "\n",
    "class DecisionStump():\n",
    "\n",
    "    def fit(self, X, y, split_features=None):        \n",
    "        \n",
    "        n, d = X.shape\n",
    "\n",
    "        # Address the trivial case where we do not split\n",
    "        count = np.bincount(y)\n",
    "\n",
    "        # Compute total entropy (needed for information gain)\n",
    "        p = count/np.sum(count); # Convert counts to probabilities\n",
    "        entropyTotal = entropy(p)\n",
    "\n",
    "        maxGain = 0\n",
    "        self.splitVariable = None\n",
    "        self.splitValue = None\n",
    "        self.splitSat = np.argmax(count)\n",
    "        self.splitNot = None\n",
    "\n",
    "        # Check if labels are not all equal\n",
    "        if np.unique(y).size <= 1:\n",
    "            return\n",
    "\n",
    "        if split_features is None:\n",
    "            split_features = range(d)\n",
    "\n",
    "        for j in split_features:\n",
    "            thresholds = np.unique(X[:,j])\n",
    "            for value in thresholds[:-1]:\n",
    "                # Count number of class labels where the feature is greater than threshold\n",
    "                y_vals = y[X[:,j] > value]\n",
    "                countSat = np.bincount(y_vals, minlength=len(count))\n",
    "                countNot = count-countSat\n",
    "                                \n",
    "                # Compute infogain\n",
    "                pSat = countSat/np.sum(countSat)\n",
    "                pNot = countNot/np.sum(countNot)\n",
    "                HSat = entropy(pSat)\n",
    "                HNot = entropy(pNot)\n",
    "                probSat = np.sum(X[:,j] > value)/n\n",
    "                probNot = 1-probSat\n",
    "                \n",
    "                infoGain = entropyTotal - probSat*HSat - probNot*HNot\n",
    "                \n",
    "                # Compare to minimum error so far\n",
    "                if infoGain > maxGain:\n",
    "                    # This is the highest information gain, store this value\n",
    "                    maxGain = infoGain\n",
    "                    splitVariable = j\n",
    "                    splitValue = value\n",
    "                    splitSat = np.argmax(countSat)\n",
    "                    splitNot = np.argmax(countNot)\n",
    "    \n",
    "        self.splitVariable = splitVariable\n",
    "        self.splitValue = splitValue\n",
    "        self.splitSat = splitSat\n",
    "        self.splitNot = splitNot\n",
    "\n",
    "    def predict(self, X):\n",
    "        splitVariable = self.splitVariable\n",
    "        splitValue = self.splitValue\n",
    "        splitSat = self.splitSat\n",
    "        splitNot = self.splitNot\n",
    "\n",
    "        m, d = X.shape\n",
    "\n",
    "        if splitVariable is None:\n",
    "            return splitSat * np.ones(m)\n",
    "\n",
    "        yhat = np.zeros(m)\n",
    "\n",
    "        for i in range(m):\n",
    "            if X[i, splitVariable] > splitValue:\n",
    "                yhat[i] = splitSat\n",
    "            else:\n",
    "                yhat[i] = splitNot\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    \"\"\"\n",
    "    Decision tree implementation in python using a\n",
    "    basic implementation of a decision stump\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_depth):\n",
    "\n",
    "        self.max_depth = max_depth\n",
    "        self.ds = DecisionStump()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fits a decision tree using greedy recursive splitting\n",
    "        \"\"\"\n",
    "        \n",
    "        # Learn a decision stump\n",
    "        self.ds.fit(X, y)\n",
    "\n",
    "        # check if we reached max depth or the decision stump splits\n",
    "        # no more, return the stum in these cases\n",
    "        if self.max_depth <= 1 or not self.ds.splitVariable:\n",
    "            self.leftModel = None\n",
    "            self.rightModel = None\n",
    "            return\n",
    "\n",
    "        j = self.ds.splitVariable\n",
    "        value = self.ds.splitValue\n",
    "\n",
    "        # Find indices of examples in each split\n",
    "        split_index_left = X[:, j] <= value\n",
    "        split_index_right = X[:, j] > value\n",
    "\n",
    "        # Fit a decision tree to each split, decreasing maximum depth by 1\n",
    "        self.leftModel = DecisionTree(self.max_depth - 1)\n",
    "        self.leftModel.fit(X[split_index_left], y[split_index_left])\n",
    "\n",
    "        self.rightModel = DecisionTree(self.max_depth - 1)\n",
    "        self.rightModel.fit(X[split_index_right], y[split_index_right])\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        n, d = X.shape\n",
    "        y = np.zeros(n)\n",
    "\n",
    "        # If no further splitting, return the majority label\n",
    "        if self.ds.splitVariable is None:\n",
    "            return self.ds.splitSat * np.ones(n)\n",
    "\n",
    "        # the case with depth=1, just a single stump\n",
    "        elif self.leftModel == None:\n",
    "            return self.ds.predict(X)\n",
    "\n",
    "        else:\n",
    "            # recurse on both the subtrees\n",
    "            j = self.ds.splitVariable\n",
    "            value = self.ds.splitValue\n",
    "\n",
    "            split_index_left = X[:, j] <= value\n",
    "            split_index_right = X[:, j] > value\n",
    "\n",
    "            y[split_index_left] = self.leftModel.predict(X[split_index_left])\n",
    "            y[split_index_right] = self.rightModel.predict(X[split_index_right])\n",
    "\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acousticness', 'danceability', 'duration_ms', 'energy', 'instrumentalness', 'key', 'liveness', 'loudness', 'mode', 'speechiness', 'tempo', 'time_signature', 'valence']\n"
     ]
    }
   ],
   "source": [
    "spotify = pd.read_csv('/Users/adityasharma/Desktop/spotifyclassification.zip', \n",
    "                      compression='zip')\n",
    "features = [x for x in spotify.columns.values if x not in ['target', 'Unnamed: 0', 'song_title', 'artist']]\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.880019831432821\n"
     ]
    }
   ],
   "source": [
    "# separating the training data and lables\n",
    "X = spotify.loc[:, features]\n",
    "y = spotify.target\n",
    "\n",
    "model_scratch_dt = DecisionTree(max_depth = 9999)\n",
    "model_scratch_dt.fit(X.values, y.values)\n",
    "\n",
    "pred = model_scratch_dt.predict(X.values)\n",
    "print(np.sum(pred == y) / y.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the decision tree classifier from the sklearn library to fit the model\n",
    "from sklearn import tree \n",
    "model = tree.DecisionTreeClassifier()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9985126425384234"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(X)\n",
    "model.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for sklearn decision tree is: 0.9985126425384234\n",
      "Training accuracy for manual decision tree is: 0.880019831432821\n"
     ]
    }
   ],
   "source": [
    "print(\"Training accuracy for sklearn decision tree is: {0}\".format(model.score(X, y)))\n",
    "print(\"Training accuracy for manual decision tree is: {0}\".format(np.sum(pred == y)/y.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.9 ms ± 187 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "# timing the sklearn decision tree\n",
    "%timeit model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.3 s ± 66.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# timing the custom made recursive decision tree\n",
    "%timeit model_scratch_dt.fit(X.values, y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
